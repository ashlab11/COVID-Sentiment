{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from autocorrect import Speller\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "tweets = pd.read_csv(\"clean_COVIDSenti.csv\")\n",
    "initial_word_dict = {}\n",
    "speller = Speller(fast=True)\n",
    "for tweet in tweets['tweet']:\n",
    "    corrected_tweet = speller(tweet)\n",
    "    for word in corrected_tweet.split(\" \"):\n",
    "        initial_word_dict[word] = initial_word_dict.get(word, 0) + 1\n",
    "\n",
    "vocab = sorted(initial_word_dict.keys(), key = initial_word_dict.get, reverse=True)\n",
    "vocab_to_int = {word: idx for idx, word in enumerate(vocab, 1)}\n",
    "\n",
    "tweets_int = []\n",
    "for tweet in tweets['tweet']:\n",
    "    corrected_tweet = speller(tweet)\n",
    "    tweets_int.append([vocab_to_int[word] for word in corrected_tweet.split()])\n",
    "    \n",
    "max_length = max(len(x) for x in tweets_int)\n",
    "\n",
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length. I changed this code from the basic code to pad AFTER\n",
    "    '''\n",
    "    ## getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "    \n",
    "    ## for each review, I grab that review\n",
    "    for i, row in enumerate(reviews_ints):\n",
    "      features[i, :len(row)] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features\n",
    "\n",
    "#Finished preprocessing tweets!\n",
    "padded_tweets = pad_features(tweets_int, max_length)\n",
    "sentiments = np.array(tweets['label']) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "#Splitting into train, val, and test sets\n",
    "split_frac = 0.9\n",
    "test_split_idx = int(len(padded_tweets) * split_frac)\n",
    "val_split_idx = int(test_split_idx * split_frac)\n",
    "x_train, y_train = padded_tweets[:val_split_idx], sentiments[:val_split_idx]\n",
    "x_val, y_val = padded_tweets[val_split_idx:test_split_idx], sentiments[val_split_idx:test_split_idx]\n",
    "x_test, y_test = padded_tweets[test_split_idx:], sentiments[test_split_idx:]\n",
    "\n",
    "\n",
    "#Turning into dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "#Dealing with imbalanced class weights for train dataset\n",
    "frequency = 1 / np.bincount(y_train)\n",
    "class_weights = torch.tensor(frequency, dtype=torch.float32)\n",
    "obs_weights = []\n",
    "for val in y_train:\n",
    "    obs_weights.append(class_weights[val])\n",
    "obs_weights = torch.tensor(obs_weights)\n",
    "train_sampler = WeightedRandomSampler(weights = obs_weights, num_samples = len(obs_weights))\n",
    "train_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, sampler = train_sampler)\n",
    "\n",
    "val_data = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))\n",
    "val_loader = DataLoader(val_data, shuffle=False, batch_size=batch_size)\n",
    "test_data = TensorDataset(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "#Get one batch for testing\n",
    "dataiter = iter(train_loader)\n",
    "sample_tweets, sample_labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Apple GPU\n"
     ]
    }
   ],
   "source": [
    "#Begin work on actual model\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Training on Apple GPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Training on CUDA\")\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    #Class used to encode positions\n",
    "    def __init__(self, embedding_dim, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-np.log(10000.0) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe) #Ensures that this positional encoding isn't updated by the optimizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe\n",
    "\n",
    "class SentimentTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, encoder_layers, nhead = 4, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encoder_layers = encoder_layers\n",
    "        \n",
    "        self.pe = PositionalEncoding(embedding_dim=embedding_dim, max_len=max_length)\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embedding_dim, nhead = nhead, dropout=dropout, batch_first=True)\n",
    "            for _ in range(encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        self.lin = nn.Linear(embedding_dim, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        mask = (input == 0) #Lets the encoder know which positions are \n",
    "        output = self.embedding(input)\n",
    "        output = self.pe(output)\n",
    "        for layer in self.encoder_layers:\n",
    "            output = layer(output, src_key_padding_mask = mask)\n",
    "        \n",
    "        output = output.mean(axis = 1)\n",
    "        output = self.lin(output)\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "#Instantiating model\n",
    "vocab_size = len(vocab_to_int)\n",
    "dropout = 0.1\n",
    "nhead = 4\n",
    "output_size = 3\n",
    "embedding_dim = 128\n",
    "encoder_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Validation set has new best accuracy tensor(0.8160, device='mps:0')\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Validation set has new best accuracy tensor(0.8491, device='mps:0')\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Validation set has new best accuracy tensor(0.8642, device='mps:0')\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n"
     ]
    }
   ],
   "source": [
    "#Training model\n",
    "# loss and optimization functions\n",
    "model = SentimentTransformer(vocab_size, output_size, embedding_dim, encoder_layers, nhead = nhead, dropout = dropout)\n",
    "lr=0.001\n",
    "counter = 0\n",
    "\n",
    "weights = class_weights.to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#Early stopping\n",
    "no_improvement = 0\n",
    "epoch = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "while no_improvement < 5:\n",
    "    epoch += 1\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    counter = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        counter += 1\n",
    "        #Pushing inputs to the correct device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output = model(inputs)\n",
    "        \n",
    "        #Calculating loss\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \"\"\"if counter % 100 == 1:\n",
    "            model.eval()\n",
    "            number_correct = torch.argmax(output, axis = 1) == labels\n",
    "            \n",
    "            correct_0 = number_correct[labels == 0]\n",
    "            correct_1 = number_correct[labels == 1]\n",
    "            accuracy = number_correct.sum() / batch_size\n",
    "            accs = []\n",
    "            sizes = []\n",
    "            for val in range(3):\n",
    "                correct_val = number_correct[labels == val]\n",
    "                acc = correct_val.sum() / len(correct_val)\n",
    "                sizes.append(len(correct_val))\n",
    "                accs.append(acc.item()) \n",
    "                \n",
    "            model.train()\n",
    "            print(f\"Loss at epoch {epoch}, counter {counter}: {loss}, accuracy {accuracy.item()}, array {accs}, sizes {sizes}\")\"\"\"\n",
    "    \n",
    "    correct = torch.tensor(0, device = device)\n",
    "    incorrect = torch.tensor(0, device = device)\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        probs = model(inputs)\n",
    "        preds = torch.argmax(probs, axis = 1)\n",
    "        preds = preds.to(device)\n",
    "        correct += (preds == labels).sum()\n",
    "        incorrect += (preds != labels).sum()  \n",
    "    accuracy = correct / (correct + incorrect)\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        print(\"Validation set has new best accuracy\", accuracy)\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8671111111111112\n"
     ]
    }
   ],
   "source": [
    "correct = torch.tensor(0, device = device)\n",
    "incorrect = torch.tensor(0, device = device)\n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    probs = model(inputs)\n",
    "    preds = torch.argmax(probs, axis = 1)\n",
    "    preds = preds.to(device)\n",
    "    correct += (preds == labels).sum()\n",
    "    incorrect += (preds != labels).sum()\n",
    "    \n",
    "print(f\"Accuracy: {correct.item() / (correct.item() + incorrect.item())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"models/SentiTrans.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
