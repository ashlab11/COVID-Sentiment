{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel, DistilBertForSequenceClassification \n",
    "\n",
    "dataset = pd.read_csv(\"clean_COVIDSenti.csv\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize(tweet):\n",
    "    tokenized = tokenizer(tweet, return_tensors='pt', padding=\"max_length\", max_length = 47) #Max tweet token length is 47\n",
    "    return tokenized\n",
    "\n",
    "tweets, labels = dataset['tweet'], dataset['label'] + 1 #Labels need to be 0-indexed\n",
    "tokenized_tweets = tweets.map(tokenize)\n",
    "tokenized_tweets, labels = tokenized_tweets.to_list(), labels.to_list()\n",
    "#print(np.min(list(map(lambda x: len(x['input_ids']), tokenized_tweets))))\n",
    "print(tokenized_tweets[0]['input_ids'])\n",
    "\n",
    "#Determining correct backend\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Training on Apple GPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Training on CUDA\")\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler, Dataset, random_split\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, tweets, labels):\n",
    "        self.x = tweets\n",
    "        self.y = labels\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Check that x is really a dictionary before processing\n",
    "        x = self.x[index]\n",
    "        x = dict(x)\n",
    "        x = {key: torch.squeeze(val, dim = 0) for key, val in x.items()}\n",
    "        y = self.y[index]\n",
    "        return (x, y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "folds = 5\n",
    "early_stopping = 5 #Stop if 5 epochs without improvement on val\n",
    "train_frac = 0.1\n",
    "test_frac = 0.8\n",
    "val_frac = 0.1\n",
    "batch_size = 64\n",
    "test_accuracies = []\n",
    "print(isinstance(labels,pd.Series))\n",
    "data = TweetDataset(tokenized_tweets, labels)\n",
    "\n",
    "\n",
    "for fold in range(folds):\n",
    "    print(f\"FOLD {fold}\")\n",
    "    gen = torch.Generator().manual_seed(fold)\n",
    "    train, val, test = random_split(data, lengths=[train_frac, val_frac, test_frac], generator=gen)\n",
    "    \n",
    "    #Dealing with imbalanced class weights for train dataset\n",
    "    labels_for_counts = list(map(lambda x: x[-1], train))\n",
    "    frequency = 1 / np.bincount(labels_for_counts)\n",
    "    class_weights = torch.tensor(frequency, dtype=torch.float32)\n",
    "    obs_weights = list(map(lambda x: class_weights[x[-1]], train))\n",
    "        \n",
    "    train_sampler = WeightedRandomSampler(weights = obs_weights, num_samples = len(obs_weights))\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, sampler = train_sampler) #Test with shuffle instead of sampler, maybe?\n",
    "    val_loader = DataLoader(val, shuffle=False, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    #---- TRAINING ACTUAL MODEL FROM HERE ON OUT ----#\n",
    "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "    model = model.to(device)\n",
    "\n",
    "    #Just train these layers to save CONSIDERABLE time -- distilbert has 6 transformer layers\n",
    "    layers = [model.classifier, model.pre_classifier, model.distilbert.transformer.layer[4], model.distilbert.transformer.layer[5]]\n",
    "    updated_params = nn.ParameterList([])\n",
    "    for layer in layers:\n",
    "        updated_params.extend(layer.parameters())\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True #Usually, this would be false, but we're updating the entirety of the model now\n",
    "        \n",
    "    for param in updated_params:\n",
    "        param.requires_grad = True\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    lr = 0.0001\n",
    "    epoch = 0\n",
    "    no_improvement = 0\n",
    "    curr_acc = 0\n",
    "    criterion = nn.CrossEntropyLoss() #Without softmax we use CEL\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = lr)\n",
    "\n",
    "    while no_improvement < early_stopping:\n",
    "        epoch += 1\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        \n",
    "        #Training model layers\n",
    "        for train_inputs, train_labels in train_loader:\n",
    "            train_inputs['input_ids'], train_inputs['attention_mask'] = train_inputs['input_ids'].to(device), train_inputs['attention_mask'].to(device)\n",
    "            train_labels = train_labels.to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            with torch.autocast(\"mps\"):\n",
    "                output = model(**train_inputs)['logits']   \n",
    "            loss = criterion(output, train_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        #Early stopping\n",
    "        model.eval()\n",
    "        correct = torch.tensor(0, device = device)\n",
    "        incorrect = torch.tensor(0, device = device)\n",
    "        \n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs['input_ids'], val_inputs['attention_mask'] = val_inputs['input_ids'].to(device), val_inputs['attention_mask'].to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "            probs = model(**val_inputs)['logits']\n",
    "            preds = torch.argmax(probs, axis = 1)\n",
    "            preds = preds.to(device)\n",
    "            correct += (preds == val_labels).sum()\n",
    "            incorrect += (preds != val_labels).sum()  \n",
    "        \n",
    "        accuracy = correct / (correct + incorrect)\n",
    "        if accuracy > curr_acc:\n",
    "            print(f\"New accuracy has been reached: {accuracy}\")\n",
    "            curr_acc = accuracy\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "    model.eval()\n",
    "    correct = torch.tensor(0, device = device)\n",
    "    incorrect = torch.tensor(0, device = device)\n",
    "    \n",
    "    #Getting test accuracy for CV purposes\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_inputs['input_ids'], test_inputs['attention_mask'] = test_inputs['input_ids'].to(device), test_inputs['attention_mask'].to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "        probs = model(**test_inputs)['logits']\n",
    "        preds = torch.argmax(probs, axis = 1)\n",
    "        preds = preds.to(device)\n",
    "        correct += (preds == test_labels).sum()\n",
    "        incorrect += (preds != test_labels).sum()  \n",
    "    \n",
    "    test_accuracy = correct / (correct + incorrect)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    print(f\"FOR FOLD {fold}, THE TEST ACCURACY WAS {test_accuracy}\")\n",
    "    print(\"---------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
