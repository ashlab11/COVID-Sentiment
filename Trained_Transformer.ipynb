{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from autocorrect import Speller\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "tweets = pd.read_csv(\"clean_COVIDSenti.csv\")\n",
    "initial_word_dict = {}\n",
    "speller = Speller(fast=True)\n",
    "for tweet in tweets['tweet']:\n",
    "    corrected_tweet = speller(tweet)\n",
    "    for word in corrected_tweet.split(\" \"):\n",
    "        initial_word_dict[word] = initial_word_dict.get(word, 0) + 1\n",
    "\n",
    "vocab = sorted(initial_word_dict.keys(), key = initial_word_dict.get, reverse=True)\n",
    "vocab_to_int = {word: idx for idx, word in enumerate(vocab, 1)}\n",
    "\n",
    "tweets_int = []\n",
    "for tweet in tweets['tweet']:\n",
    "    corrected_tweet = speller(tweet)\n",
    "    tweets_int.append([vocab_to_int[word] for word in corrected_tweet.split()])\n",
    "    \n",
    "max_length = max(len(x) for x in tweets_int)\n",
    "print(max_length)\n",
    "\n",
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length. I changed this code from the basic code to pad AFTER\n",
    "    '''\n",
    "    ## getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "    \n",
    "    ## for each review, I grab that review\n",
    "    for i, row in enumerate(reviews_ints):\n",
    "      features[i, :len(row)] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features\n",
    "\n",
    "#Finished preprocessing tweets!\n",
    "padded_tweets = pad_features(tweets_int, max_length)\n",
    "sentiments = np.array(tweets['label']) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "#Splitting into train, val, and test sets\n",
    "split_frac = 0.9\n",
    "test_split_idx = int(len(padded_tweets) * split_frac)\n",
    "val_split_idx = int(test_split_idx * split_frac)\n",
    "x_train, y_train = padded_tweets[:val_split_idx], sentiments[:val_split_idx]\n",
    "x_val, y_val = padded_tweets[val_split_idx:test_split_idx], sentiments[val_split_idx:test_split_idx]\n",
    "x_test, y_test = padded_tweets[test_split_idx:], sentiments[test_split_idx:]\n",
    "\n",
    "\n",
    "#Turning into dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "#Dealing with imbalanced class weights for train dataset\n",
    "frequency = 1 / np.bincount(y_train)\n",
    "class_weights = torch.tensor(frequency, dtype=torch.float32)\n",
    "obs_weights = []\n",
    "for val in y_train:\n",
    "    obs_weights.append(class_weights[val])\n",
    "obs_weights = torch.tensor(obs_weights)\n",
    "train_sampler = WeightedRandomSampler(weights = obs_weights, num_samples = len(obs_weights))\n",
    "train_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, sampler = train_sampler)\n",
    "\n",
    "val_data = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))\n",
    "val_loader = DataLoader(val_data, shuffle=False, batch_size=batch_size)\n",
    "test_data = TensorDataset(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Apple GPU\n"
     ]
    }
   ],
   "source": [
    "#Begin work on actual model\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Training on Apple GPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Training on CUDA\")\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    #Class used to encode positions\n",
    "    def __init__(self, embedding_dim, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-np.log(10000.0) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe) #Ensures that this positional encoding isn't updated by the optimizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe\n",
    "\n",
    "class SentimentTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, encoder_layers, nhead = 4, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encoder_layers = encoder_layers\n",
    "        \n",
    "        self.pe = PositionalEncoding(embedding_dim=embedding_dim, max_len=max_length)\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embedding_dim, nhead = nhead, dropout=dropout, batch_first=True)\n",
    "            for _ in range(encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        self.lin = nn.Linear(embedding_dim, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        mask = (input == 0) #Lets the encoder know which positions are \n",
    "        output = self.embedding(input)\n",
    "        output = self.pe(output)\n",
    "        for layer in self.encoder_layers:\n",
    "            output = layer(output, src_key_padding_mask = mask)\n",
    "        \n",
    "        output = output.mean(axis = 1)\n",
    "        output = self.lin(output)\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "#Instantiating model\n",
    "vocab_size = len(vocab_to_int)\n",
    "dropout = 0.1\n",
    "nhead = 4\n",
    "output_size = 3\n",
    "embedding_dim = 128\n",
    "encoder_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Validation set has new best accuracy tensor(0.8290, device='mps:0')\n",
      "0.004910927906371\n",
      "Epoch 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     41\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 42\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"if counter % 100 == 1:\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m        model.eval()\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m        number_correct = torch.argmax(output, axis = 1) == labels\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m        model.train()\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m        print(f\"Loss at epoch {epoch}, counter {counter}: {loss}, accuracy {accuracy.item()}, array {accs}, sizes {sizes}\")\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m correct \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0\u001b[39m, device \u001b[38;5;241m=\u001b[39m device)\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:  \u001b[38;5;66;03m# noqa: D105\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefaults\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults,\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate,\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparam_groups\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups,\n\u001b[1;32m    389\u001b[0m     }\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# noqa: D105\u001b[39;00m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(state)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_optimizer_step_pre_hooks\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m     75\u001b[0m prev_grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# Note on graph break below:\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# we need to graph break to ensure that aot respects the no_grad annotation.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# This is important for perf because without this, functionalization will generate an epilogue\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# which updates the mutated parameters of the optimizer which is *not* visible to inductor, as a result,\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# inductor will allocate for every parameter in the model, which is horrible.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# With this, aot correctly sees that this is an inference graph, and functionalization will generate\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# an epilogue which is appended to the graph, which *is* visible to inductor, as a result, inductor sees that\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# step is in place and is able to avoid the extra allocation.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# In the future, we will either 1) continue to graph break on backward, so this graph break does not matter\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# or 2) have a fully fused forward and backward graph, which will have no_grad by default, and we can remove this\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# graph break to allow the fully fused fwd-bwd-optimizer graph to be compiled.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# see https://github.com/pytorch/pytorch/issues/104053\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    160\u001b[0m     _device_dtype_check_for_fused(p)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# note(crcrpar): [special device hosting for step]\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Deliberately host `step` on CPU if both capturable and fused are off.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# This is because kernel launches are costly on CUDA and XLA.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    165\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    166\u001b[0m         (),\n\u001b[1;32m    167\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m_get_scalar_dtype(is_fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m--> 168\u001b[0m         device\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m_get_scalar_dtype())\n\u001b[1;32m    172\u001b[0m )\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[1;32m    174\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[1;32m    175\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[1;32m    176\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/torch/optim/adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    244\u001b[0m             adam(\n\u001b[1;32m    245\u001b[0m                 params_with_grad,\n\u001b[1;32m    246\u001b[0m                 grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m                 found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    265\u001b[0m             )\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m    270\u001b[0m Adam\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Implements Adam algorithm.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m    .. math::\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m       \\begin{aligned}\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m            &\\rule{110mm}{0.4pt}                                                                 \\\\\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m            &\\textbf{input}      : \\gamma \\text{ (lr)}, \\beta_1, \\beta_2\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m                \\text{ (betas)},\\theta_0 \\text{ (params)},f(\\theta) \\text{ (objective)}          \\\\\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m            &\\hspace{13mm}      \\lambda \\text{ (weight decay)},  \\: \\textit{amsgrad},\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m                \\:\\textit{maximize},  \\: \\epsilon \\text{ (epsilon)}                              \\\\\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m            &\\textbf{initialize} :  m_0 \\leftarrow 0 \\text{ ( first moment)},\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m                v_0\\leftarrow 0 \\text{ (second moment)},\\: \\widehat{v_0}^{max}\\leftarrow 0\\\\[-1.ex]\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m            &\\rule{110mm}{0.4pt}                                                                 \\\\\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m            &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m            &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m            &\\hspace{10mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})         \\\\\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m            &\\hspace{5mm}\\textbf{else}                                                           \\\\\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m            &\\hspace{10mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m            &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m            &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m            &\\hspace{5mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m            &\\hspace{5mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m            &\\hspace{5mm}\\widehat{m_t} \\leftarrow   m_t/\\big(1-\\beta_1^t \\big)                   \\\\\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m            &\\hspace{5mm}\\widehat{v_t} \\leftarrow   v_t/\\big(1-\\beta_2^t \\big)                   \\\\\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m            &\\hspace{5mm}\\textbf{if} \\: amsgrad                                                  \\\\\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m            &\\hspace{10mm}\\widehat{v_t}^{max} \\leftarrow \\mathrm{max}(\\widehat{v_{t-1}}^{max},\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m                \\widehat{v_t})                                                                   \\\\\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m            &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m                \\big(\\sqrt{\\widehat{v_t}^{max}} + \\epsilon \\big)                                 \\\\\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;03m            &\\hspace{5mm}\\textbf{else}                                                           \\\\\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m            &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03m                \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big)                                       \\\\\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m            &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m            &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m            &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m       \\end{aligned}\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    For further details regarding the algorithm we refer to `Adam: A Method for Stochastic Optimization`_.\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124m    Args:\u001b[39m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_params_doc\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124m        lr (float, Tensor, optional): learning rate (default: 1e-3). A tensor LR\u001b[39m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124m            is not yet supported for all our implementations. Please use a float\u001b[39m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124m            LR if you are not also specifying fused=True or capturable=True.\u001b[39m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124m        betas (Tuple[float, float], optional): coefficients used for computing\u001b[39m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124m            running averages of gradient and its square (default: (0.9, 0.999))\u001b[39m\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;124m        eps (float, optional): term added to the denominator to improve\u001b[39m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124m            numerical stability (default: 1e-8)\u001b[39m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124m        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\u001b[39m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124m        amsgrad (bool, optional): whether to use the AMSGrad variant of this\u001b[39m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;124m            algorithm from the paper `On the Convergence of Adam and Beyond`_\u001b[39m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;124m            (default: False)\u001b[39m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_foreach_doc\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_maximize_doc\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_capturable_doc\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_differentiable_doc\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_fused_doc\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124m    .. Note::\u001b[39m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124m        A prototype implementation of Adam and AdamW for MPS supports `torch.float32` and `torch.float16`.\u001b[39m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124m    .. _Adam\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m: A Method for Stochastic Optimization:\u001b[39m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124m        https://arxiv.org/abs/1412.6980\u001b[39m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124m    .. _On the Convergence of Adam and Beyond:\u001b[39m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124m        https://openreview.net/forum?id=ryQu7f-RZ\u001b[39m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    337\u001b[0m )\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_single_tensor_adam\u001b[39m(\n\u001b[1;32m    341\u001b[0m     params: List[Tensor],\n\u001b[1;32m    342\u001b[0m     grads: List[Tensor],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m     differentiable: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    360\u001b[0m ):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m grad_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m found_inf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/torch/optim/adam.py:441\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n\u001b[0;32m--> 441\u001b[0m         max_exp_avg_sq \u001b[38;5;241m=\u001b[39m max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    443\u001b[0m         max_exp_avg_sq \u001b[38;5;241m=\u001b[39m max_exp_avg_sqs[i]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "#Training model\n",
    "# loss and optimization functions\n",
    "model = SentimentTransformer(vocab_size, output_size, embedding_dim, encoder_layers, nhead = nhead, dropout = dropout)\n",
    "lr=0.001\n",
    "counter = 0\n",
    "\n",
    "weights = class_weights.to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#Early stopping\n",
    "no_improvement = 0\n",
    "epoch = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "while no_improvement < 5:\n",
    "    epoch += 1\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    counter = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        counter += 1\n",
    "        #Pushing inputs to the correct device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output = model(inputs)\n",
    "        \n",
    "        #Calculating loss\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        start = time.time()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    correct = torch.tensor(0, device = device)\n",
    "    incorrect = torch.tensor(0, device = device)\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        probs = model(inputs)\n",
    "        preds = torch.argmax(probs, axis = 1)\n",
    "        preds = preds.to(device)\n",
    "        correct += (preds == labels).sum()\n",
    "        incorrect += (preds != labels).sum()  \n",
    "    accuracy = correct / (correct + incorrect)\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        print(\"Validation set has new best accuracy\", accuracy)\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8671111111111112\n"
     ]
    }
   ],
   "source": [
    "correct = torch.tensor(0, device = device)\n",
    "incorrect = torch.tensor(0, device = device)\n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    probs = model(inputs)\n",
    "    preds = torch.argmax(probs, axis = 1)\n",
    "    preds = preds.to(device)\n",
    "    correct += (preds == labels).sum()\n",
    "    incorrect += (preds != labels).sum()\n",
    "    \n",
    "print(f\"Accuracy: {correct.item() / (correct.item() + incorrect.item())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"models/SentiTrans.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
